{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Tokenize : Exercise-1 with Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Python NLTK program to split the text sentence/paragraph into a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sarath/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "\n",
      "Joe waited for the train. The train was late. \n",
      "Mary and Samantha took the bus. \n",
      "I looked for Mary and Samantha at the bus station.\n",
      "\n",
      "\n",
      "Sentence-tokenized copy in a list:\n",
      "['\\nJoe waited for the train.', 'The train was late.', 'Mary and Samantha took the bus.', 'I looked for Mary and Samantha at the bus station.']\n",
      "\n",
      "Read the list:\n",
      "\n",
      "Joe waited for the train.\n",
      "The train was late.\n",
      "Mary and Samantha took the bus.\n",
      "I looked for Mary and Samantha at the bus station.\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "Joe waited for the train. The train was late. \n",
    "Mary and Samantha took the bus. \n",
    "I looked for Mary and Samantha at the bus station.\n",
    "'''\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "token_text = sent_tokenize(text)\n",
    "print(\"\\nSentence-tokenized copy in a list:\")\n",
    "print(token_text)\n",
    "print(\"\\nRead the list:\")\n",
    "for s in token_text:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Python NLTK program to tokenize sentences in languages other than English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "\n",
      "NLTK ist Open Source Software. Der Quellcode wird unter den Bedingungen der Apache License Version 2.0 vertrieben.  \n",
      "Die Dokumentation wird unter den Bedingungen der Creative Commons-Lizenz Namensnennung - Nicht kommerziell - Keine \n",
      "abgeleiteten Werke 3.0 in den Vereinigten Staaten verteilt.\n",
      "\n",
      "\n",
      "Sentence-tokenized copy in a list:\n",
      "['\\nNLTK ist Open Source Software.', 'Der Quellcode wird unter den Bedingungen der Apache License Version 2.0 vertrieben.', 'Die Dokumentation wird unter den Bedingungen der Creative Commons-Lizenz Namensnennung - Nicht kommerziell - Keine \\nabgeleiteten Werke 3.0 in den Vereinigten Staaten verteilt.']\n",
      "\n",
      "Read the list:\n",
      "\n",
      "NLTK ist Open Source Software.\n",
      "Der Quellcode wird unter den Bedingungen der Apache License Version 2.0 vertrieben.\n",
      "Die Dokumentation wird unter den Bedingungen der Creative Commons-Lizenz Namensnennung - Nicht kommerziell - Keine \n",
      "abgeleiteten Werke 3.0 in den Vereinigten Staaten verteilt.\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "NLTK ist Open Source Software. Der Quellcode wird unter den Bedingungen der Apache License Version 2.0 vertrieben.  \n",
    "Die Dokumentation wird unter den Bedingungen der Creative Commons-Lizenz Namensnennung - Nicht kommerziell - Keine \n",
    "abgeleiteten Werke 3.0 in den Vereinigten Staaten verteilt.\n",
    "'''\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "token_text = sent_tokenize(text, language='german')\n",
    "print(\"\\nSentence-tokenized copy in a list:\")\n",
    "print(token_text)\n",
    "print(\"\\nRead the list:\")\n",
    "for s in token_text:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Python NLTK program to create a list of words from a given string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution code 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "Joe waited for the train. The train was late. Mary and Samantha took the bus. I looked for Mary and Samantha at the bus station.\n",
      "\n",
      "List of words:\n",
      "['Joe', 'waited', 'for', 'the', 'train', '.', 'The', 'train', 'was', 'late', '.', 'Mary', 'and', 'Samantha', 'took', 'the', 'bus', '.', 'I', 'looked', 'for', 'Mary', 'and', 'Samantha', 'at', 'the', 'bus', 'station', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Joe waited for the train. The train was late. Mary and Samantha took the bus. I looked for Mary and Samantha at the bus station.\"\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)\n",
    "print(\"\\nList of words:\")\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution Code 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "Joe waited for the train. The train was late. Mary and Samantha took the bus. I looked for Mary and Samantha at the bus station.\n",
      "\n",
      "List of words:\n",
      "['Joe', 'waited', 'for', 'the', 'train.', 'The', 'train', 'was', 'late.', 'Mary', 'and', 'Samantha', 'took', 'the', 'bus.', 'I', 'looked', 'for', 'Mary', 'and', 'Samantha', 'at', 'the', 'bus', 'station', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "text = \"Joe waited for the train. The train was late. Mary and Samantha took the bus. I looked for Mary and Samantha at the bus station.\"\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)\n",
    "print(\"\\nList of words:\")\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a Python NLTK program to split all punctuation into separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "Reset your password if you just can't remember your old one.\n",
      "\n",
      "Split all punctuation into separate tokens:\n",
      "['Reset', 'your', 'password', 'if', 'you', 'just', 'can', \"'\", 't', 'remember', 'your', 'old', 'one', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "text = \"Reset your password if you just can't remember your old one.\"\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)\n",
    "result = WordPunctTokenizer().tokenize(text)\n",
    "print(\"\\nSplit all punctuation into separate tokens:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a Python NLTK program to tokenize words, sentence wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "Joe waited for the train. The train was late. Mary and Samantha took the bus. I looked for Mary and Samantha at the bus station.\n",
      "\n",
      "Tokenize words sentence wise:\n",
      "\n",
      "Read the list:\n",
      "['Joe', 'waited', 'for', 'the', 'train', '.']\n",
      "['The', 'train', 'was', 'late', '.']\n",
      "['Mary', 'and', 'Samantha', 'took', 'the', 'bus', '.']\n",
      "['I', 'looked', 'for', 'Mary', 'and', 'Samantha', 'at', 'the', 'bus', 'station', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "text = \"Joe waited for the train. The train was late. Mary and Samantha took the bus. I looked for Mary and Samantha at the bus station.\"\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)\n",
    "print(\"\\nTokenize words sentence wise:\")\n",
    "result = [word_tokenize(t) for t in sent_tokenize(text)]\n",
    "print(\"\\nRead the list:\")\n",
    "for s in result:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Write a Python NLTK program to tokenize a twitter text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Tweet:\n",
      "NoSQL introduction - w3resource http://bit.ly/1ngHC5F  #nosql #database #webdev\n",
      "\n",
      "Tokenize a twitter text:\n",
      "['NoSQL', 'introduction', '-', 'w3resource', 'http://bit.ly/1ngHC5F', '#nosql', '#database', '#webdev']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tweet_text = \"NoSQL introduction - w3resource http://bit.ly/1ngHC5F  #nosql #database #webdev\"\n",
    "print(\"\\nOriginal Tweet:\")\n",
    "print(tweet_text)\n",
    "result = tknzr.tokenize(tweet_text)\n",
    "print(\"\\nTokenize a twitter text:\")\n",
    "print(result) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a Python NLTK program to remove Twitter username handles from a given twitter text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Tweet:\n",
      "@abcd @pqrs NoSQL introduction - w3resource http://bit.ly/1ngHC5F  #nosql #database #webdev\n",
      "\n",
      "Tokenize a twitter text:\n",
      "['NoSQL', 'introduction', '-', 'w3resource', 'http://bit.ly/1ngHC5F', '#nosql', '#database', '#webdev']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True)\n",
    "tweet_text = \"@abcd @pqrs NoSQL introduction - w3resource http://bit.ly/1ngHC5F  #nosql #database #webdev\"\n",
    "print(\"\\nOriginal Tweet:\")\n",
    "print(tweet_text)\n",
    "result = tknzr.tokenize(tweet_text)\n",
    "print(\"\\nTokenize a twitter text:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a Python NLTK program that will read a given text through each line and look for sentences. Print each sentence and divide two sentences with “==============”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution code 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Tweet:\n",
      "\n",
      "Mr. Smith waited for the train. The train was late.\n",
      "Mary and Samantha took the bus. I looked for Mary and\n",
      "Samantha at the bus station.\n",
      "\n",
      "Mr. Smith waited for the train.\n",
      "==============\n",
      "The train was late.\n",
      "==============\n",
      "Mary and Samantha took the bus.\n",
      "==============\n",
      "I looked for Mary and\n",
      "Samantha at the bus station.\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "text = '''\n",
    "Mr. Smith waited for the train. The train was late.\n",
    "Mary and Samantha took the bus. I looked for Mary and\n",
    "Samantha at the bus station.\n",
    "'''\n",
    "print(\"\\nOriginal Tweet:\")\n",
    "print(text)\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "print('\\n==============\\n'.join(sent_detector.tokenize(text.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution Code2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Tweet:\n",
      "\n",
      "Mr. Smith waited for the train. (The train was late.)\n",
      "Mary and Samantha took the bus. I looked for Mary and\n",
      "Samantha at the bus station [Sector-1].\n",
      "\n",
      "Mr. Smith waited for the train.\n",
      "==============\n",
      "(The train was late.)\n",
      "==============\n",
      "Mary and Samantha took the bus.\n",
      "==============\n",
      "I looked for Mary and\n",
      "Samantha at the bus station [Sector-1].\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "text = '''\n",
    "Mr. Smith waited for the train. (The train was late.)\n",
    "Mary and Samantha took the bus. I looked for Mary and\n",
    "Samantha at the bus station [Sector-1].\n",
    "'''\n",
    "print(\"\\nOriginal Tweet:\")\n",
    "print(text)\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "print('\\n==============\\n'.join(sent_detector.tokenize(text.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a Python NLTK program to find parenthesized expressions in a given string and divides the string into a sequence of substrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Tweet:\n",
      "(a b (c d)) e f (g)\n",
      "['(a b (c d))', 'e', 'f', '(g)']\n",
      "\n",
      "Original Tweet:\n",
      "(a b) (c d) e (f g)\n",
      "['(a b)', '(c d)', 'e', '(f g)']\n",
      "\n",
      "Original Tweet:\n",
      "[(a b (c d)) e f (g)]\n",
      "['[', '(a b (c d))', 'e', 'f', '(g)', ']']\n",
      "[(a b (c d)) e f (g)]\n",
      "['[', '(a b (c d))', 'e', 'f', '(g)', ']']\n",
      "\n",
      "Original Tweet:\n",
      "{a b {c d}} e f {g}\n",
      "['{a b {c d}} e f {g}']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import SExprTokenizer\n",
    "text = '(a b (c d)) e f (g)'\n",
    "print(\"\\nOriginal Tweet:\")\n",
    "print(text)\n",
    "print(SExprTokenizer().tokenize(text))\n",
    "text = '(a b) (c d) e (f g)'\n",
    "print(\"\\nOriginal Tweet:\")\n",
    "print(text)\n",
    "print(SExprTokenizer().tokenize(text))\n",
    "text = '[(a b (c d)) e f (g)]'\n",
    "print(\"\\nOriginal Tweet:\")\n",
    "print(text)\n",
    "print(SExprTokenizer().tokenize(text))\n",
    "print(text)\n",
    "print(SExprTokenizer().tokenize(text))\n",
    "text = '{a b {c d}} e f {g}'\n",
    "print(\"\\nOriginal Tweet:\")\n",
    "print(text)\n",
    "print(SExprTokenizer().tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
