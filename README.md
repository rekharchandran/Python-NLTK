# Python-NLTK-Tokenize

Tokenization is the process of demarcating and possibly classifying sections of a string of input characters. 

The resulting tokens are then passed on to some other form of processing. The process can be considered a sub-task of parsing input.




