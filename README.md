# Python-NLTK-Tokenize
Tokenization is the process of demarcating and possibly classifying sections of a string of input characters. 

The resulting tokens are then passed on to some other form of processing. The process can be considered a sub-task of parsing input.

Tokenization is a way to split text into tokens. These tokens could be paragraphs, sentences, or individual words.

NLTK provides a number of tokenizers in the tokenize module.NLTK provides a function called word_tokenize() for splitting strings into tokens (nominally words). It splits tokens based on white space and punctuation.


