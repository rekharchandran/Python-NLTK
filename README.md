# Python-NLTK-Tokenize
Tokenization is the process of demarcating and possibly classifying sections of a string of input characters.
