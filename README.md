# Python-NLTK-Tokenize
Tokenization is the process of demarcating and possibly classifying sections of a string of input characters. The resulting tokens are then passed on to some other form of processing. The process can be considered a sub-task of parsing input.
Tokenization is a way to split text into tokens. 
These tokens could be paragraphs, sentences, or individual words. NLTK provides a number of tokenizers in the tokenize module.
The text is first tokenized into sentences using the PunktSentenceTokenizer.

